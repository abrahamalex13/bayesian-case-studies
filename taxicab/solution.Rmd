---
title: "The Taxicab Problem"
date: "11/6/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center')

do_main <- FALSE
if (do_main) {
  source("main.R")
}
viz_priors <- readRDS("viz_priors.rds")
```

# Problem Statement

"Suppose you arrive in a new city and see a taxi numbered 100. How many taxis are there in this city?"^[I find this question in Kevin Murphy's _Machine Learning: A Probabilistic Perspective_. A great read!] Taxis are numbered 0, 1, 2, 3, ..., up to the last taxi.


# A Classical Approach

You already have intuition about how to proceed. The likelihood of what you observed -- taxicab #100 -- depends on how many taxis there are in total. Your estimate of total taxi count should somehow elevate the likelihood of what you observed.

A classical approach: estimate a total taxi count which _maximizes_ the likelihood of what you observed.^["Maximum Likelihood Estimation" is a fundamental method in statistics.] An estimate of 101 follows. (Recall: numbering started at zero.) Why? Assuming the city has 101 taxicabs, the likelihood of observing taxicab #100 equals 1/101, or about 1%. Estimate any higher number of taxicabs, and this probability necessarily decreases.

However, we should be unsatisfied with this estimate! This "optimal" estimate follows from a _narrowly-defined_ framework. The framework must be scrutinized. Then, a fundamental question arises: what if we simply observed an event with less-than-maximum likelihood? This feels entirely possible -- our lives are full of relatively surprising events. 

To robustify our estimate, we may take an alternate statistical approach.


# A Bayesian Improvement

Consider this estimation approach: let's specify prior knowledge about total taxi count, and then update that assessment, given what we observed (taxicab #100).^[This estimation approach follows Bayes' Theorem, another fundamental statement in statistics.]

Question one: how might we specify prior knowledge about total taxi count? First we'll identify the range of possible estimates. Then, we'll assign each possible value a weight -- higher weight signifies a more probable estimate. The weights may be generated by a well-defined probability formula^[In the lingo, probability distribution.]. Envision a few scenarios for prior knowledge^[These prior knowledge scenarios come from one probability distribution _family_, the Pareto. This particular family makes for simpler calculations later.]:

```{r}
  viz_priors
```


