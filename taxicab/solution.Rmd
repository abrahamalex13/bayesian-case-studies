---
title: "The Taxicab Problem"
date: "11/6/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center')

do_main <- TRUE
if (do_main) {
  source("main.R")
}
viz_priors <- readRDS("viz_priors.rds")
```

# Problem Statement

"Suppose you arrive in a new city and see a taxi numbered 100. How many taxis are there in this city?"^[I find this question in Kevin Murphy's _Machine Learning: A Probabilistic Perspective_. A great read!] Taxis are numbered 0, 1, 2, 3, ..., up to the last taxi.


# A Classical Approach

You already have intuition about how to proceed. The likelihood of the observed data -- taxicab #100 -- depends on how many taxis there are in total. Your estimate of total taxi count should somehow elevate the likelihood of what you observed.

A classical approach: estimate a total taxi count which _maximizes_ the likelihood of the observed data.^["Maximum Likelihood Estimation" is a fundamental method in statistics.] Under this approach, we estimate 101 total taxis. (Numbering started at zero.) Why? When the city has 101 taxis, the likelihood of observing taxicab #100 equals 1/101. This probability decreases from its maximum if the taxi estimate rises above 101.


# Classical (Frequentist) Approach May Be Too Narrow for Our Problem

**The classical estimate follows from a narrow approach, which may or may not address our true problem.** Suppose we want to quantify uncertainty in our estimate--or hypothesis--given the data we observe. The classical approach does not directly accommodate that calculation. The limitation traces back to the classical approach's fundamental definition of uncertainty/probability: event frequency over _repeated trials_. Hence the methodology name, "frequentism". 

Classical estimation uncertainty quantifies probability of data--which in theory, we could repeatedly sample--given a hypothesis about the world. An analyst might say: "if this idea about the world were true, then I'm unlikely to observe these data or some still more extreme." 

Importantly, frequentists do not quantify probability of a hypothesis--a single event--given data. For this calculation, we must employ a Bayesian approach.


# A Bayesian Approach

Suppose that we fundamentally define probability as, 
our "strength of belief" in possible outcomes for an event.
Now we can quantify the probability of a hypothesis, given data.

TODO ---

In the taxicab problem context, we:

1. Specify prior knowledge about total taxi count, and then 
2. Update that assessment, given what we observed (taxicab #100).

## Specify Prior Knowledge About Total Taxi Count

How might we specify prior knowledge about total taxi count? We'll quantify our "strength of belief" in possible values. 

- First we'll identify that complete set of possible values. 
- Then, we'll assign a weight to each possible value -- higher weight signifies stronger belief. The weights may be generated by an explicit probability formula^[In the lingo, probability distribution.], for simpler calculations later. 

Imagine a few scenarios for prior knowledge^[These prior knowledge scenarios come from one probability distribution _family_, the Pareto.]:

```{r}
  viz_priors
```


## Update Belief About Total Taxi Count, Given Observed Evidence



