---
title: "A Drift Detection Building Block: Kullback-Leibler Divergence"
date: "3/15/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center')

```

A key problem in data analysis, especially a data analysis _system_: how to automatically detect data drift? 
KL Divergence is a fast, automatic measure of dissimilarity between distributions.
But "how large is too large", thus indicating drift?
Solve that problem with _classical statistics_.
With a simple formula, and classical statistics framework,
we have a building block for automated drift detection.

# KL Divergence Measures Dissimilarity Between Probability Distributions

Suppose we have two probability distributions (a distribution defines
the likelihood of generating a data value, over all possible values).
To measure the distributions' dissimilarity, we may:

- compute the difference between their probabilities,
- weight, and
- sum

This procedure formalizes in Kullback-Leibler (KL) Divergence--
which intakes probability distributions _p_ and _q_,
and partitions data values by segment _k_:

$$
  KL = \sum_{k=1}^K p_k log \frac{p_k}{q_k}
$$

The measure rises as the two distributions assign increasingly different probability to a data segment. And that difference matters more for high-probability segments of _p_. Regarding special cases: the measure equals zero when the distributions are identical, or it approaches infinity when a data segment has probability zero under _q_ but nonzero under _p_.

KL Divergence is a fast calculation of distributions' dissimilarity.
But how do we make meaning of a KL Divergence value? 
What's a large value or a small value? 

# First Define Monitoring Window, then KL Divergence Test Structure Follows
## Case Example: Cars