---
title: "Automated Drift Detection Building Block: KL Divergence"
date: "3/15/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center')

DIR_PLOTS <- paste0("figures") 

```

A key problem in data analysis, especially a data analysis _system_: 
how to automatically detect data drift? 
KL Divergence is a fast, automatic measure of dissimilarity between distributions.
But "how large is too large", thus indicating drift?

Solve that problem with _classical statistics_--hypothesis testing, specifically.
First establish the question: does one probability distribution 
differ (drift) from a baseline probability distribution? 
To assess, we need the range of feasible values when that condition is true.
Called the statistic's _sampling distribution_.

We can estimate the sampling 
under the condition of no data drift, 
simulate the range of feasible KL Divergence values, through data re-sampling. 
Then, measure/contextualize a new value relative to that feasible range. 
Estimate the probability of a value still more extreme.
When that probability ("p-value") is low, then reject a baseline hypothesis of no-drift.

With a simple formula, and classical statistics framework,
we have a building block for automated drift detection.

# KL Divergence Measures Dissimilarity Between Probability Distributions

Suppose we have two probability distributions (a distribution defines
the likelihood of generating a data value, over all possible values).
To measure the distributions' dissimilarity, we may:

- compute the difference between their probabilities,
- weight, and
- sum

This procedure formalizes in Kullback-Leibler (KL) Divergence--
which intakes probability distributions _p_ and _q_,
and partitions data values by segment _k_:

$$
  KL = \sum_{k=1}^K p_k log \frac{p_k}{q_k}
$$

As a data segment's probability increasingly differs between the two distributions,
the measure rises. And that difference matters more for high-probability segments. 
Regarding special cases: the measure equals zero when the distributions are identical, 
or it approaches infinity when a data segment has probability zero under _q_ but nonzero under _p_.

So KL Divergence is a fast calculation of distributions' dissimilarity.
But how do we make meaning of a KL Divergence value? 
What's a large value or a small value? 

# For Data Drift Test, Measure KL Divergence Relative to Other No-Drift Scenarios

In data drift testing, KL Divergence's meaning is _relative_--
relative to its range of values under no-drift scenarios. 
Even under no data drift, KL Divergence should vary from zero,
due to randomness in data sampling. 
And with smaller data samples, KL Divergence should vary more widely.
We can estimate the range of values under no data drift,
by simulating repeated data samples.

KL Divergence's relative value provides evidence for the question: 
how much does one probability distribution drift from a baseline distribution? 
If among no-drift scenarios, a more extreme KL is rarely seen,
then we question the premise of no data drift.

## "First Principles" Testing Approach Parallels Classical Hypothesis Testing

With intuition and first principles, we've formulated a drift test from KL Divergence.
Classical hypothesis testing may frame our specific steps:

- Declare a question about _population(s)_ underlying data 
  - Does one data distribution drift from another?
- Propose a baseline (null) hypothesis, to be tested
  - Assume no data drift--setting up comparison to other no-drift scenarios
- Devise a test statistic, having known range of values under correct null hypothesis (having known _sampling distribution_)
  - Simulate the range of KL Divergence values under no drift 
- Calculate test statistic for observed data
  - Calculate KL Divergence between test and baseline distributions
- Measure test statistic relative to its range of values under correct null hypothesis. May compute _p-value_: probability of a test statistic with equal or greater extremity, assuming correct null hypothesis.
  - Measure KL Divergence relative to its values in no-drift scenarios
- Evaluate null hypothesis's reasonableness, according to implied likelihood of observed data
  - If among no-drift scenarios, a more extreme KL is rarely seen, then question the premise of no data drift.

# Case Example: Cars

We've articulated a sound method. Let's apply it.

PASTE CARVANA DATA DETAIL FROM FORMER BLOG

We compare a test set's probability distribution to the historical distribution.
Because we estimate each distribution by summarizing data observations,
our results are sensitive to observation counts. In the results that follow,
we'll vary test set observation count, and always use the entire historical data.
Considering the entire data: there are about 49,000 test observations, 
and about 72,000 historical observations.

## Visualize Distributions Estimated from Entire Data

If we estimate the test set probability distribution from the entire data --  
we might suspect an insignificant drift in distribution, 
without precise domain knowledge to tell us otherwise:

```{r}
  knitr::include_graphics(paste0(DIR_PLOTS, "/drift_viz_overall.png"))
```

## Visualize Extreme Distribution Under No Drift, n=49K Test Set

How can we devise an automated test to align with the entire data implication,
and flag us when there's _significant_ drift? 
_We must tune test set nobs, which yields the KL Divergence feasible values under no drift._ 
If we choose test set nobs too high, our drift test will be too sensitive.
We should not expect real data reflect precisely zero drift -- 
the data-generating conditions are complex, dynamic, and likely time-varying.
This thought process parallels a broader theme in statistical testing
for distributional alignment^[https://www.allendowney.com/blog/2023/01/28/never-test-for-normality/, 
https://allendowney.github.io/ElementsOfDataScience/anderson.html].
So we must tune our testing to reflect _practically significant_ drift,
above _statistically significant_ drift.

```{r}
  knitr::include_graphics(paste0(DIR_PLOTS, "/drift_viz_boot_naive.png"))
```

## Visualize Extreme Distribution Under No Drift, n=100 Test Set

When we tune test set nobs to a level that allows wider variation in KL Divergence,
we arrive at a test that captures _practically significant_ drift. 
Now we'd be alerted when the extent of drift looks like below:

```{r}
  knitr::include_graphics(paste0(DIR_PLOTS, "/drift_viz_boot_tuned.png"))
```