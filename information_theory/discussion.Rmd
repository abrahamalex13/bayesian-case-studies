---
title: "A Drift Detection Building Block: Kullback-Leibler Divergence"
date: "3/15/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center')

viz_priors <- readRDS("viz_priors.rds")
viz_priors_posteriors <- readRDS("viz_priors_posteriors.rds")
viz_posterior_pred <- readRDS("viz_posterior_pred.rds")
```

A key problem in data analysis, especially a data analysis _system_: how to automatically detect data drift? 
KL Divergence is a fast, automatic measure of dissimilarity between distributions.
But "how large is too large", thus indicating drift?
Solve that problem with _classical statistics_.
With a simple formula, and classical statistics framework,
we have a building block for automated drift detection.

# KL Divergence Measures Dissimilarity Between Distributions
# First Define Monitoring Window, then KL Divergence Test Structure Follows
## Case Example: Cars