---
title: "Automated Drift Detection Building Block: KL Divergence"
date: "3/15/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center')

DIR_PLOTS <- paste0("figures") 

```

A key problem in data analysis, especially a data analysis _system_: 
how to automatically detect data drift? 
KL Divergence is a fast, automatic measure of dissimilarity between distributions.
But "how large is too large", thus indicating drift?

Solve that problem with _classical statistics_--hypothesis testing, specifically.
First establish the question: does one probability distribution 
differ (drift) from a baseline probability distribution? 
To assess, we need the range of feasible values when that condition is true.
Called the statistic's _sampling distribution_.

We can estimate the sampling 
under the condition of no data drift, 
simulate the range of feasible KL Divergence values, through data re-sampling. 
Then, measure/contextualize a new value relative to that feasible range. 
Estimate the probability of a value still more extreme.
When that probability ("p-value") is low, then reject a baseline hypothesis of no-drift.

With a simple formula, and classical statistics framework,
we have a building block for automated drift detection.


How can we devise an automated test to align with the entire data implication,
and flag us when there's _significant_ drift? 
_We must tune test set nobs, which yields the KL Divergence feasible values under no drift._ 
If we choose test set nobs too high, our drift test will be too sensitive.


# A Data Drift Case Study: Lemons at Carvana

Given two datasets, (1) test and (2) historical -- let's assess data distribution _drift_. How dissimilar are the test and historical _population distributions_, in light of the sample evidence? Conclusions depend on the test dataset's observation count. In smaller data samples, there's more variability in the drift statistic (KL Divergence, described later). *Ultimately we'll tune an automated method to detect meaningful data drifts.*

Let's study real data from Carvana.
PASTE CARVANA DATA DETAIL FROM FORMER BLOG.
In total, Carvana shares about 49,000 test observations and 73,000 historical observations.

Specifically, let's assess drift in car's Make.

## Manually Assess Distribution Drift, with Entire Data

Let's first eyeball the extent of drift, using the entire data. 
This manual assessment helps tune an automated method for live future monitoring.

The probability distribution appears (visually) similar, test versus historical:

```{r}
  knitr::include_graphics(paste0(DIR_PLOTS, "/drift_viz_overall.png"))
```

Without deeper domain knowledge to suggest otherwise,
let's adopt the ground truth of no meaningful data drift.
The same assessment should arise from a properly-tuned automated method.

## Statistically, Test Data Have Drifted (As Expected)

For an automated method to detect no data drift,
a test sample must compare similarly to other samples under no population drift.
Even under no _population_ data drift, 
data _samples_ will reflect a range of drift.

We can simulate samples under no population drift,
by repeated draws direct from historical data.
Using the samples' drift statistics (KL Divergence, described later), 
we can visualize extreme possible samples,
and score relative extremity of a new test sample.

Turns out, the observed test sample drifts more from historical
than practically any equal-size sample under no population drift.
Under no population drift, an equal-size test sample will rarely drift more than below:

```{r}
  knitr::include_graphics(paste0(DIR_PLOTS, "/drift_viz_boot_naive.png"))
```

Real data--generated by dynamic processes--
might be expected to drift, technically speaking.
This theme also appears in other statistical tests of distributions^[See https://www.allendowney.com/blog/2023/01/28/never-test-for-normality/, or
https://allendowney.github.io/ElementsOfDataScience/anderson.html].
So we must tune our tests to detect _practically significant_ drift,
above _statistically significant_ drift.

## Practical Drift Detection Tunes to Appropriate Sensitivity

A drift detection method will be less sensitive if
it learns no-drift scenarios with smaller data samples.
In smaller data samples, the drift measure should vary more widely.
Ultimately we tune to a method that captures _practically significant_ drift. 
Now we'd be alerted when drift extremity meets or exceeds below:

```{r}
  knitr::include_graphics(paste0(DIR_PLOTS, "/drift_viz_boot_tuned.png"))
```

We've arrived at an algorithm which flags drift at an appropriate threshold,
_useful for live future monitoring_.

# KL Divergence Measures Dissimilarity Between Probability Distributions

Suppose we have two probability distributions (a distribution defines
the likelihood of generating a data value, over all possible values).
To measure the distributions' dissimilarity, we may:

- compute the difference between their probabilities,
- weight, and
- sum

This procedure formalizes in Kullback-Leibler (KL) Divergence--
which intakes probability distributions _p_ and _q_,
and partitions data values by segment _k_:

$$
  KL = \sum_{k=1}^K p_k log \frac{p_k}{q_k}
$$

As a data segment's probability increasingly differs between the two distributions,
the measure rises. And that difference matters more for high-probability segments. 
Regarding special cases: the measure equals zero when the distributions are identical, 
or it approaches infinity when a data segment has probability zero under _q_ but nonzero under _p_.

So KL Divergence is a fast calculation of distributions' dissimilarity.
But how do we make meaning of a KL Divergence value? 
What's a large value or a small value? 


# For Data Drift Test, Measure KL Divergence Relative to Other No-Drift Scenarios

In data drift testing, KL Divergence's meaning is _relative_--
relative to its range of values under no-drift scenarios. 
Even under no data drift, KL Divergence should vary from zero,
due to randomness in data sampling. 
And with smaller data samples, KL Divergence should vary more widely.
We can estimate the range of values under no data drift,
by simulating repeated data samples.

KL Divergence's relative value provides evidence for the question: 
how much does one probability distribution drift from a baseline distribution? 
If among no-drift scenarios, a more extreme KL is rarely seen,
then we question the premise of no data drift.

## "First Principles" Testing Approach Parallels Classical Hypothesis Testing

With intuition and first principles, we've formulated a drift test from KL Divergence.
Classical hypothesis testing may frame our specific steps:

- Declare a question about _population(s)_ underlying data 
  - Does one data distribution drift from another?
- Propose a baseline (null) hypothesis, to be tested
  - Assume no data drift--setting up comparison to other no-drift scenarios
- Devise a test statistic, having known range of values under correct null hypothesis (having known _sampling distribution_)
  - Simulate the range of KL Divergence values under no drift 
- Calculate test statistic for observed data
  - Calculate KL Divergence between test and baseline distributions
- Measure test statistic relative to its range of values under correct null hypothesis. May compute _p-value_: probability of a test statistic with equal or greater extremity, assuming correct null hypothesis.
  - Measure KL Divergence relative to its values in no-drift scenarios
- Evaluate null hypothesis's reasonableness, according to implied likelihood of observed data
  - If among no-drift scenarios, a more extreme KL is rarely seen, then question the premise of no data drift.
