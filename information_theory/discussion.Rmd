---
title: "Automated Drift Detection Building Block: KL Divergence"
date: "3/15/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center')

```

A key problem in data analysis, especially a data analysis _system_: 
how to automatically detect data drift? 
KL Divergence is a fast, automatic measure of dissimilarity between distributions.
But "how large is too large", thus indicating drift?

Solve that problem with _classical statistics_--hypothesis testing, specifically.
First establish the question: does one probability distribution 
differ (drift) from a baseline probability distribution? 
To assess, we need the range of feasible values when that condition is true.
Called the statistic's _sampling distribution_.

We can estimate the sampling 
under the condition of no data drift, 
simulate the range of feasible KL Divergence values, through data re-sampling. 
Then, measure/contextualize a new value relative to that feasible range. 
Estimate the probability of a value still more extreme.
When that probability ("p-value") is low, then reject a baseline hypothesis of no-drift.

With a simple formula, and classical statistics framework,
we have a building block for automated drift detection.

# KL Divergence Measures Dissimilarity Between Probability Distributions

Suppose we have two probability distributions (a distribution defines
the likelihood of generating a data value, over all possible values).
To measure the distributions' dissimilarity, we may:

- compute the difference between their probabilities,
- weight, and
- sum

This procedure formalizes in Kullback-Leibler (KL) Divergence--
which intakes probability distributions _p_ and _q_,
and partitions data values by segment _k_:

$$
  KL = \sum_{k=1}^K p_k log \frac{p_k}{q_k}
$$

As a data segment's probability increasingly differs between the two distributions,
the measure rises. And that difference matters more for high-probability segments. 
Regarding special cases: the measure equals zero when the distributions are identical, 
or it approaches infinity when a data segment has probability zero under _q_ but nonzero under _p_.

So KL Divergence is a fast calculation of distributions' dissimilarity.
But how do we make meaning of a KL Divergence value? 
What's a large value or a small value? 

# KL Divergence Meaning is Relative to Distribution Under No Data Drift

When testing for data drift, KL Divergence's meaning is _relative_--
relative to the range of feasible values under no data drift. 
Even under no data drift, KL Divergence should vary from zero,
due to randomness in data sampling. 
And when working with smaller samples, KL Divergence should vary more widely.
Fortunately we can estimate the range of feasible values under no data drift,
by simulating repeated data samples.

Given a KL Divergence statistic, we ask: 
does one probability distribution differ (drift) from the baseline distribution? 
If _among the no-data-drift feasible values_,
a more extreme statistic is improbable,
then we challenge the premise of no data drift.

The above intuition reflects classical statistics hypothesis testing:
- we established a question about the data population
- we assumed an answer (no drift between the two probability distributions)
- we calculated a test statistic (as well as its feasible values, or "sampling distribution", under no drift)
- on the condition of the assumed answer, we calculated conditional probability ("p-value") of a still-more-extreme test statistic
- we challenged our assumption if it implied an observed outcome too improbable


# Case Example: Cars

We've articulated a sound method. Let's apply it.

PASTE CARVANA DATA DETAIL FROM FORMER BLOG

We compare a 49K-obs test set of car makes to the 72K-obs historical distribution of car makes.

The overall visualization suggests, visually insignificant drift in make distribution (admitting uncertainty about precise business interpretation)

CHART

However, 