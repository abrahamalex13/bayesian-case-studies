---
title: "Automated Drift Detection Building Block: KL Divergence"
date: "3/15/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center')

```

A key problem in data analysis, especially a data analysis _system_: how to automatically detect data drift? 
KL Divergence is a fast, automatic measure of dissimilarity between distributions.
But "how large is too large", thus indicating drift?

Solve that problem with _classical statistics_--hypothesis testing, specifically.
First, establish the question: does one probability distribution 
differ (drift) from a baseline probability distribution? 
To assess, we need the range of feasible values when that condition is true.
Called the statistic's _sampling distribution_.
(
    When a new observations test set does truly generate from
    population probability distribution q, 
    KL Divergence sampling variation partly controlled by:
        - Test set sample size (small sample size, wider variation)
)

We can estimate the sampling 
under the condition of no data drift, 
simulate the range of feasible KL Divergence values, through data re-sampling. 
Then, measure/contextualize a new value relative to that feasible range. 
Estimate the probability of a value still more extreme.
When that probability ("p-value") is low, then reject a baseline hypothesis of no-drift.


With a simple formula, and classical statistics framework,
we have a building block for automated drift detection.

# KL Divergence Measures Dissimilarity Between Probability Distributions

Suppose we have two probability distributions (a distribution defines
the likelihood of generating a data value, over all possible values).
To measure the distributions' dissimilarity, we may:

- compute the difference between their probabilities,
- weight, and
- sum

This procedure formalizes in Kullback-Leibler (KL) Divergence--
which intakes probability distributions _p_ and _q_,
and partitions data values by segment _k_:

$$
  KL = \sum_{k=1}^K p_k log \frac{p_k}{q_k}
$$

As a data segment's probability increasingly differs between the two distributions,
the measure rises. And that difference matters more for high-probability segments. 
Regarding special cases: the measure equals zero when the distributions are identical, 
or it approaches infinity when a data segment has probability zero under _q_ but nonzero under _p_.

KL Divergence is a fast calculation of distributions' dissimilarity.
But how do we make meaning of a KL Divergence value? 
What's a large value or a small value? 

# To Assess KL Divergence, Use Distribution Under No Data Drift

# Assuming No Data Drift, What's the Probability of this KLD or Yet More Extreme?



## Case Example: Cars